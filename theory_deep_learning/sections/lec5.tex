\paragraph{Over-parametrization case} Most of the previous work rely on \textit{small networks} or put some restrictions on activation functions. Let's now consider the case when there is overparametrization i.e. hidden layer size $k$ > data points $m$ and the activation used is the one that is practically used - \textit{ReLU}.

\paragraph{Solving ERM}
Assume some data points in 2-d of the form $S = \{(x^i,y^i), ... , (x^m,y^m)\}$. Assume 2 layer Neural Network with $k$ hidden units and 1 output unit.

So $f(x_i) = \sum_{i=1}^{k}a_iReLU((<w,x> + b_i))$

We need to find parameters $w_i, b_i, a_i$ such that $f(x_i) = y_i$ for all $i \in [m]$.

\paragraph{Proof by construction} Let all $w_i$ be in a \textit{special direction} with unit norm. This means that $w_i = w$. The special direction is such that if you take a line perpendicular to that direction, you'll encounter at most one data point over the whole 2-d space. \textcolor{red}{Prove that such a line always exists.} Let's pass our $x$ through the Neural Network.



\begin{flalign}
	& f(x^1) = a_1ReLU((<w,x^1> + b_1)) + a_2ReLU((<w,x^1> + b_2)) + ...\\
	& f(x^2) = a_1ReLU((<w,x^2> + b_1)) + a_2ReLU((<w,x^2> + b_2)) + ...\\
	& f(x^3) = a_1ReLU((<w,x^3> + b_1)) + a_2ReLU((<w,x^3> + b_2)) + a_3ReLU((<w,x^3> + b_3)) ...
\end{flalign}

Let's say I choose $b_1$ such that $ReLU((<w,x_1> + b_1)) = \epsilon$ and choose $a_i = y^i/\epsilon$. Then $ReLU((<w,x_1> + b_1)) = 0$ for $x$ lying beyond $\epsilon$ left of $x^1$ since $<w,x>$ is just the projection of $x$ on $w$. Also value of the NN at $x^1$ will then be $y^1$.

Now consider $x^2$. Choose $b_2$ in similar fashion i.e. $ReLU((<w,x_2> + b_2)) = 0$ for all $x$ beyond $\epsilon$ left of $x_2$. The first term in (2) will contribute let's say $p$. Let output of the second term in (2) be $a_2p$ then you can choose $a_2$ such that $a_2p + q = y^2$. No other term will contribute to this equation since we'll choose their $b_i$ so that they are zero to the $\epsilon$ left of respective $x_i$.

Note that, for a successful construction, you assumed that there is at least some small $\epsilon$ distance along $w$ between two points.

\textcolor{red}{Need figure.}

\paragraph{Solving ERM proves nothing} We showed that we can solve for ERM for $k \geq m$. Now, since $L_\mathcal{D}(\mathcal{H}) \leq L_S(\mathcal{H}) + O\big(\sqrt{\frac{VC(\mathcal{H}) + log(1/\delta)}{m}}\big)$, the second term is greater than 1. \textcolor{red}{How?}. But we knew that $L_\mathcal{D}(\mathcal{H})$ can be at most 1 so, this is a \textit{vacuous bound}. This is just an example of the larger observation that traditional bounds fail for the overparametrization case. And if you were to use such an ERM hypothesis in practice, you'll observe a large generalization error.